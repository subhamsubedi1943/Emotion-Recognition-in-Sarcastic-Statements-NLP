{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "path = 'model_output.h5'\n",
    "my_reloaded_model = tf.keras.models.load_model(\n",
    "       (path),\n",
    "       custom_objects={'KerasLayer':hub.KerasLayer}\n",
    ")\n",
    "result = \"\"\n",
    "def predict_res(sentence):\n",
    "        reviews = [sentence]\n",
    "        result = my_reloaded_model.predict(reviews)\n",
    "        return result\n",
    "#print(predict_res(\"not a sarcasm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in C:\\Users\\ANKIT\\AppData\\Local\\Temp\\temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "sarcasm\n"
     ]
    }
   ],
   "source": [
    "#working with text input and dark mode\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "import moviepy.editor as mp\n",
    "import simpleaudio as sa\n",
    "from transformers import pipeline\n",
    "from PIL import Image, ImageTk\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "class VideoTranscriberApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Emotion and Sarcasm Detection\")\n",
    "\n",
    "        # Set dark mode colors\n",
    "        background_color = \"#1E1E1E\"  # Dark gray background\n",
    "        button_color = \"#3E3E3E\"  # Darker gray for buttons\n",
    "        text_color = \"#FFFFFF\"  # White text\n",
    "\n",
    "        self.master.configure(bg=background_color)\n",
    "\n",
    "        # Button row\n",
    "        self.button_row = tk.Frame(master, bg=background_color)\n",
    "        self.button_row.pack()\n",
    "\n",
    "\n",
    "\n",
    "        # Select video, audio, and transcribe buttons\n",
    "        self.select_video_button = tk.Button(self.button_row, text=\"Select Video\", command=self.select_video, bg=button_color, fg=text_color)\n",
    "        self.select_video_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.select_audio_button = tk.Button(self.button_row, text=\"Select Audio\", command=self.select_audio, bg=button_color, fg=text_color)\n",
    "        self.select_audio_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.transcribe_button = tk.Button(self.button_row, text=\"Predict\", command=self.transcribe, bg=button_color, fg=text_color)\n",
    "        self.transcribe_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.function_var = tk.StringVar()\n",
    "        self.function_var.set(\"Models\")\n",
    "\n",
    "        # Use a themed Combobox for better appearance\n",
    "        self.function_dropdown = ttk.Combobox(master, textvariable=self.function_var, values=[\"DistilBert\", \"Bert\"])\n",
    "        self.function_dropdown.pack(padx=5, pady=10)\n",
    "        self.function_dropdown.configure(style=\"Dark.TCombobox\")\n",
    "\n",
    "        # Style for the Combobox\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')  # Choose an available theme\n",
    "\n",
    "        style.configure(\"TCombobox\", fieldbackground=button_color, foreground=text_color)\n",
    "        style.map(\"TCombobox\", fieldbackground=[('readonly', button_color)])\n",
    "\n",
    "        # Labels and other widgets\n",
    "        self.file_path_label = tk.Label(master, text=\"Selected File:\", bg=background_color, fg=text_color)\n",
    "        self.file_path_label.pack()\n",
    "\n",
    "        self.transcription_label = tk.Label(master, text=\"Transcription:\", width=60, height=10, wraplength=400, bg=background_color, fg=text_color)\n",
    "        self.transcription_label.pack(padx=10, pady=10)\n",
    "\n",
    "        # User text input box\n",
    "        self.user_text_entry = tk.Entry(master, width=60, bg=button_color, fg=text_color)\n",
    "        self.user_text_entry.pack(pady=10, padx=10)\n",
    "\n",
    "        self.play_button = tk.Button(master, text=\"Play File\", command=self.load_file, bg=button_color, fg=text_color)\n",
    "        self.play_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Transcribe text button\n",
    "        self.transcribe_text_button = tk.Button(self.button_row, text=\"Predict(TEXT)\", command=self.transcribe_text, bg=button_color, fg=text_color)\n",
    "        self.transcribe_text_button.pack(pady=10, padx=5)\n",
    "        \n",
    "        # Video player\n",
    "        self.video_player = ttk.Frame(master, style=\"Dark.TFrame\")\n",
    "        self.video_player.pack()\n",
    "        self.canvas = tk.Canvas(self.video_player, width=640, height=480, bg=background_color)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # Function execution messages\n",
    "        self.function_message_label = tk.Label(master, text=\"\", font=(\"Helvetica\", 12), bg=background_color, fg=text_color)\n",
    "        self.function_message_label.pack(pady=10)\n",
    "\n",
    "        # File path and type\n",
    "        self.file_path = None\n",
    "        self.file_type = None\n",
    "\n",
    "\n",
    "\n",
    "    def select_video(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Video Files\", \"*.mp4;*.avi\")])\n",
    "        self.file_path_label.config(text=\"Selected File: \" + file_path)\n",
    "        self.file_path = file_path\n",
    "        self.file_type = \"video\"\n",
    "\n",
    "    def select_audio(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Audio Files\", \"*.wav;*.mp3\")])\n",
    "        self.file_path_label.config(text=\"Selected File: \" + file_path)\n",
    "        self.file_path = file_path\n",
    "        self.file_type = \"audio\"\n",
    "\n",
    "    def transcribe(self):\n",
    "        if self.file_path:\n",
    "            if self.file_type == \"video\":\n",
    "                transcription = self.transcribe_video(self.file_path)\n",
    "            elif self.file_type == \"audio\":\n",
    "                transcription = self.transcribe_audio(self.file_path)\n",
    "            else:\n",
    "                transcription = \"Unsupported file type. Please select a valid video or audio file.\"\n",
    "            self.transcription_label.config(text=\"Transcription: \" + transcription)\n",
    "        else:\n",
    "            self.transcription_label.config(text=\"Please select a file first.\")\n",
    "\n",
    "    def transcribe_text(self):\n",
    "        user_text = self.user_text_entry.get()\n",
    "        if user_text:\n",
    "            self.transcription_label.config(text=\"Transcription: \" + user_text)\n",
    "            self.execute_function(user_text)\n",
    "        else:\n",
    "            self.transcription_label.config(text=\"Please enter text first.\")\n",
    "\n",
    "    def transcribe_video(self, video_path):\n",
    "        audio_path = self.extract_audio(video_path)\n",
    "        transcription = self.transcribe_audio_pipeline(audio_path)\n",
    "        self.execute_function(transcription)\n",
    "        return transcription\n",
    "\n",
    "    def transcribe_audio(self, audio_path):\n",
    "        transcription = self.transcribe_audio_pipeline(audio_path)\n",
    "        return transcription\n",
    "\n",
    "    def transcribe_audio_pipeline(self, audio_path):\n",
    "        cls = pipeline(\"automatic-speech-recognition\")\n",
    "        transcription = cls(audio_path)\n",
    "        transcription = transcription[\"text\"]\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    def extract_audio(self, video_path):\n",
    "        video_clip = mp.VideoFileClip(video_path)\n",
    "        audio_clip = video_clip.audio\n",
    "\n",
    "        # Specify an absolute path for the temporary audio file\n",
    "        temp_audio_file = os.path.join(tempfile.gettempdir(), \"temp_audio.wav\")\n",
    "\n",
    "        audio_clip.write_audiofile(temp_audio_file)\n",
    "        audio_clip.close()  # Close the audio clip to release the file\n",
    "\n",
    "        return temp_audio_file\n",
    "\n",
    "    def load_file(self):\n",
    "        if self.file_path:\n",
    "            if self.file_type == \"video\":\n",
    "                self.load_video()\n",
    "            elif self.file_type == \"audio\":\n",
    "                self.play_audio(self.file_path)\n",
    "            else:\n",
    "                print(\"Unsupported file type. Please select a valid video or audio file.\")\n",
    "\n",
    "    def load_video(self):\n",
    "        video_clip = mp.VideoFileClip(self.file_path)\n",
    "        self.video_clip = video_clip\n",
    "        self.audio_clip = video_clip.audio\n",
    "        self.play_file()\n",
    "\n",
    "    def play_audio(self, audio_path):\n",
    "        # Load and play the audio using simpleaudio\n",
    "        wave_obj = sa.WaveObject.from_wave_file(audio_path)\n",
    "        play_obj = wave_obj.play()\n",
    "\n",
    "        # Sleep for the duration of the audio\n",
    "        time.sleep(play_obj.duration)\n",
    "\n",
    "        play_obj.stop()  # Stop audio playback when the audio ends\n",
    "\n",
    "    def play_file(self):\n",
    "        # Set canvas size to match video dimensions\n",
    "        self.canvas.config(width=self.video_clip.size[0], height=self.video_clip.size[1])\n",
    "\n",
    "        # Load and play the audio using simpleaudio\n",
    "        wave_obj = sa.WaveObject.from_wave_file(self.extract_audio(self.file_path))\n",
    "        play_obj = wave_obj.play()\n",
    "\n",
    "        for frame in self.video_clip.iter_frames(fps=self.video_clip.fps, dtype='uint8'):\n",
    "            img = Image.fromarray(frame)\n",
    "            img = ImageTk.PhotoImage(img)\n",
    "            self.canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "            self.canvas.image = img\n",
    "            self.master.update()\n",
    "            time.sleep(0.6 / self.video_clip.fps)  # Adjust the sleep duration based on the desired speed\n",
    "\n",
    "        play_obj.stop()  # Stop audio playback when the video ends\n",
    "\n",
    "    def execute_function(self, msg):\n",
    "        selected_function = self.function_var.get()\n",
    "\n",
    "        if selected_function == \"DistilBert\":\n",
    "            word = msg\n",
    "            results = predict_res(word)\n",
    "            op = HuggingFace_pred(word)\n",
    "            text1 = \"Result: sarcasm with \"+op\n",
    "            text2 = \"Result: Not sarcasm with \"+op\n",
    "            if results > 0.3:\n",
    "                print(\"sarcasm\")\n",
    "                self.function_message_label.config(text=text1, fg=\"#FF4500\")  # Orange text for sarcasm\n",
    "            else:\n",
    "                print(\"not sarcasm\")\n",
    "                self.function_message_label.config(text=text2, fg=\"#32CD32\")  # Green text for not sarcasm\n",
    "\n",
    "        elif selected_function == \"Bert\":\n",
    "            reviews = [msg]\n",
    "             \n",
    "            res = bert_model.predict(reviews)\n",
    "            print(res)\n",
    "            if res > 0.3:\n",
    "                print(\"sarcasm\")\n",
    "                self.function_message_label.config(text=\"Sarcasm\", fg=\"#FF4500\")  # Orange text for sarcasm\n",
    "            else:\n",
    "                print(\"not sarcasm\")\n",
    "                self.function_message_label.config(text=\"Not Sarcasm\", fg=\"#32CD32\")  # Green text for not sarcasm\n",
    "\n",
    "            #self.function_message_label.config(text=\"model 2.\", fg=\"#008B8B\")  # Dark cyan text for Model 2\n",
    "        else:\n",
    "            self.function_message_label.config(text=\"Select a Model!.\", fg=\"#FF0000\")  # Red text for invalid function\n",
    "\n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Dark theme style\n",
    "    style = ttk.Style(root)\n",
    "    style.theme_use('clam')\n",
    "    style.configure('Dark.TFrame', background='#1E1E1E')  # Dark gray background for frames\n",
    "    style.configure('Dark.TCombobox', background='#3E3E3E', foreground='#FFFFFF')  # Darker gray for Combobox\n",
    "\n",
    "    app = VideoTranscriberApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "from datasets import load_dataset\n",
    "emotion = load_dataset('emotion')\n",
    "emotion.set_format(type='pandas')\n",
    "classes = emotion['train'].features['label'].names\n",
    "#classes\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "num_labels = len(classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels = num_labels).to(device)\n",
    "def HuggingFace_pred(Trans):  \n",
    "  text = Trans\n",
    "  input_encoded = tokenizer(text, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**input_encoded)\n",
    "\n",
    "  logits = outputs.logits\n",
    "  pred = torch.argmax(logits, dim=1).item()\n",
    "  op = classes[pred]\n",
    "  return op\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./NLP-Tutorials-with-HuggingFace/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "path = \"bertMustard++test.h5\"\n",
    "bert_model = tf.keras.models.load_model(\n",
    "       (path),\n",
    "       custom_objects={'KerasLayer':hub.KerasLayer},\n",
    "       compile=False\n",
    ")\n",
    "bert_model.compile() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
